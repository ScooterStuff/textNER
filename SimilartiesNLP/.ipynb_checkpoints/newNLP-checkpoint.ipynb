{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c1e686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.training import Example\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "from spacy.util import filter_spans\n",
    "import random\n",
    "from pathlib import Path\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72d571de",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = [\n",
    "    (\"I want to play Minecraft\", {\"entities\": [(15, 24, \"GAME\")]}),\n",
    "    (\"Start playing Minecraft with my right hand\", {\"entities\": [(14, 23, \"GAME\"), (32, 42, \"HAND\")]}),\n",
    "    (\"place a block down\", {\"entities\": []}),\n",
    "    (\"I love Minecraft\", {\"entities\": []}),\n",
    "    (\"hold my fist to place a block down\", {\"entities\": [(8, 12, \"GESTURE\")]}),\n",
    "    (\"Let's play Rocket League\", {\"entities\": [(11, 24, \"GAME\")]}),\n",
    "    (\"I use the joystick to drive in Horizon\", {\"entities\": [(31, 38, \"GAME\"), (22, 27, \"GESTURE\")]}),\n",
    "    (\"FIFA is best played with a controller\", {\"entities\": [(0, 4, \"GAME\"), (27, 37, \"GESTURE\")]}),\n",
    "    (\"Playing Tetris requires quick rotations\", {\"entities\": [(8, 14, \"GAME\")]}),\n",
    "    (\"Cast a spell in Final Fantasy\", {\"entities\": [(16, 29, \"GAME\")]}),\n",
    "    (\"Score a goal in FIFA using my feet\", {\"entities\": [(16, 20, \"GAME\")]}),\n",
    "    (\"Build a tower in Minecraft with blocks\", {\"entities\": [(17, 26, \"GAME\")]}),\n",
    "    (\"Launch the ball in Rocket League with a flip\", {\"entities\": [(19, 32, \"GAME\")]}),\n",
    "    (\"Rotate pieces in Tetris quickly\", {\"entities\": [(17, 23, \"GAME\"), (0, 6, \"GESTURE\")]}),\n",
    "    (\"Drive through the field in Horizon with speed\", {\"entities\": [(27, 34, \"GAME\"), (0, 5, \"GESTURE\")]}),\n",
    "    (\"Perform a free kick in FIFA\", {\"entities\": [(23, 27, \"GAME\"), (10, 19, \"GESTURE\")]}),\n",
    "    (\"Summon your ally in Final Fantasy\", {\"entities\": [(20, 33, \"GAME\")]}),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b25c1a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the pre-trained 'en_core_web_sm' model\n",
      "Losses at iteration 0: {'ner': 38.84356198094276}\n",
      "Losses at iteration 1: {'ner': 37.36295401562617}\n",
      "Losses at iteration 2: {'ner': 35.138318952281}\n",
      "Losses at iteration 3: {'ner': 31.302118829939445}\n",
      "Losses at iteration 4: {'ner': 28.114366880750822}\n",
      "Losses at iteration 5: {'ner': 22.810956803546105}\n",
      "Losses at iteration 6: {'ner': 19.101992157757024}\n",
      "Losses at iteration 7: {'ner': 22.699482739957016}\n",
      "Losses at iteration 8: {'ner': 19.316348836047005}\n",
      "Losses at iteration 9: {'ner': 19.348964436276777}\n",
      "Losses at iteration 10: {'ner': 15.071955375100451}\n",
      "Losses at iteration 11: {'ner': 10.314000480588433}\n",
      "Losses at iteration 12: {'ner': 12.355362414821819}\n",
      "Losses at iteration 13: {'ner': 6.429380289445243}\n",
      "Losses at iteration 14: {'ner': 6.787670726955305}\n",
      "Losses at iteration 15: {'ner': 6.288120121424518}\n",
      "Losses at iteration 16: {'ner': 5.009038514117714}\n",
      "Losses at iteration 17: {'ner': 3.152630977118878}\n",
      "Losses at iteration 18: {'ner': 5.320777913500605}\n",
      "Losses at iteration 19: {'ner': 4.834994730260172}\n",
      "Losses at iteration 20: {'ner': 1.980473078086412}\n",
      "Losses at iteration 21: {'ner': 7.7659670424689144}\n",
      "Losses at iteration 22: {'ner': 3.6948267715152943}\n",
      "Losses at iteration 23: {'ner': 2.3773115669613336}\n",
      "Losses at iteration 24: {'ner': 4.294770733803869}\n",
      "Losses at iteration 25: {'ner': 4.673584143538116}\n",
      "Losses at iteration 26: {'ner': 6.518472417565518}\n",
      "Losses at iteration 27: {'ner': 4.528975342805767}\n",
      "Losses at iteration 28: {'ner': 1.696109014940941}\n",
      "Losses at iteration 29: {'ner': 1.398600987893418}\n",
      "Losses at iteration 30: {'ner': 3.112695711155687}\n",
      "Losses at iteration 31: {'ner': 2.7998117731483667}\n",
      "Losses at iteration 32: {'ner': 1.0477363913126985}\n",
      "Losses at iteration 33: {'ner': 2.0894638573288753}\n",
      "Losses at iteration 34: {'ner': 1.959156926290779}\n",
      "Losses at iteration 35: {'ner': 2.2886893018304297}\n",
      "Losses at iteration 36: {'ner': 1.4293835066844116}\n",
      "Losses at iteration 37: {'ner': 1.283533358386573}\n",
      "Losses at iteration 38: {'ner': 2.7690926046209516}\n",
      "Losses at iteration 39: {'ner': 1.0788165419242963}\n",
      "Losses at iteration 40: {'ner': 0.16380052581719295}\n",
      "Losses at iteration 41: {'ner': 0.02869195795781139}\n",
      "Losses at iteration 42: {'ner': 2.068679334279613}\n",
      "Losses at iteration 43: {'ner': 2.0432330657345927}\n",
      "Losses at iteration 44: {'ner': 1.6674857450961675}\n",
      "Losses at iteration 45: {'ner': 0.26020443024193685}\n",
      "Losses at iteration 46: {'ner': 2.1231751073211798}\n",
      "Losses at iteration 47: {'ner': 0.21172131156973675}\n",
      "Losses at iteration 48: {'ner': 0.5895889704877519}\n",
      "Losses at iteration 49: {'ner': 0.00014070771847895863}\n",
      "Losses at iteration 50: {'ner': 0.0006682101528755572}\n",
      "Losses at iteration 51: {'ner': 0.11703399646044037}\n",
      "Losses at iteration 52: {'ner': 0.0005529279674945362}\n",
      "Losses at iteration 53: {'ner': 0.04885918037730384}\n",
      "Losses at iteration 54: {'ner': 7.414961546180199e-05}\n",
      "Losses at iteration 55: {'ner': 0.016665241433088465}\n",
      "Losses at iteration 56: {'ner': 0.00013950739702458856}\n",
      "Losses at iteration 57: {'ner': 0.1218226762736733}\n",
      "Losses at iteration 58: {'ner': 2.0183857750090817}\n",
      "Losses at iteration 59: {'ner': 0.2291927437860937}\n",
      "Losses at iteration 60: {'ner': 0.0009084566311509116}\n",
      "Losses at iteration 61: {'ner': 0.2574791678084732}\n",
      "Losses at iteration 62: {'ner': 0.15250717669287964}\n",
      "Losses at iteration 63: {'ner': 8.35410435490792e-06}\n",
      "Losses at iteration 64: {'ner': 1.9900468632099613}\n",
      "Losses at iteration 65: {'ner': 0.0002915123212964562}\n",
      "Losses at iteration 66: {'ner': 0.269358918382941}\n",
      "Losses at iteration 67: {'ner': 0.014544240310569391}\n",
      "Losses at iteration 68: {'ner': 0.00011748099594434519}\n",
      "Losses at iteration 69: {'ner': 0.00014858851313394542}\n",
      "Losses at iteration 70: {'ner': 0.0011479847367398298}\n",
      "Losses at iteration 71: {'ner': 0.0034735065500914254}\n",
      "Losses at iteration 72: {'ner': 0.0002499815351068979}\n",
      "Losses at iteration 73: {'ner': 0.0002619260003228584}\n",
      "Losses at iteration 74: {'ner': 0.0001945944531871256}\n",
      "Losses at iteration 75: {'ner': 0.004511965606576149}\n",
      "Losses at iteration 76: {'ner': 2.972618494631141e-05}\n",
      "Losses at iteration 77: {'ner': 1.9623997876440746}\n",
      "Losses at iteration 78: {'ner': 0.0001845269203535926}\n",
      "Losses at iteration 79: {'ner': 2.2870880075185065}\n",
      "Losses at iteration 80: {'ner': 0.011563909784794503}\n",
      "Losses at iteration 81: {'ner': 2.8506358181023965e-05}\n",
      "Losses at iteration 82: {'ner': 4.982696710822729e-06}\n",
      "Losses at iteration 83: {'ner': 3.575164985500925e-05}\n",
      "Losses at iteration 84: {'ner': 0.003851280313925307}\n",
      "Losses at iteration 85: {'ner': 0.00020569100566884222}\n",
      "Losses at iteration 86: {'ner': 0.000648057442953548}\n",
      "Losses at iteration 87: {'ner': 4.264239867451998e-06}\n",
      "Losses at iteration 88: {'ner': 1.5084606084069963}\n",
      "Losses at iteration 89: {'ner': 2.280150321687842e-07}\n",
      "Losses at iteration 90: {'ner': 1.5948823358493307e-05}\n",
      "Losses at iteration 91: {'ner': 4.446160905109196}\n",
      "Losses at iteration 92: {'ner': 1.0856979296547572e-07}\n",
      "Losses at iteration 93: {'ner': 0.8701023238263745}\n",
      "Losses at iteration 94: {'ner': 0.40951269936883117}\n",
      "Losses at iteration 95: {'ner': 0.009105428999406607}\n",
      "Losses at iteration 96: {'ner': 1.4962362888973868}\n",
      "Losses at iteration 97: {'ner': 5.466608244330258e-06}\n",
      "Losses at iteration 98: {'ner': 5.581126177148685e-05}\n",
      "Losses at iteration 99: {'ner': 2.2783463927998808e-07}\n",
      "Losses at iteration 100: {'ner': 0.00039630280121579636}\n",
      "Losses at iteration 101: {'ner': 2.2027439284813144e-06}\n",
      "Losses at iteration 102: {'ner': 0.0005660306179389398}\n",
      "Losses at iteration 103: {'ner': 0.03749899195580846}\n",
      "Losses at iteration 104: {'ner': 0.00011778624680739698}\n",
      "Losses at iteration 105: {'ner': 7.482509028912683e-08}\n",
      "Losses at iteration 106: {'ner': 3.240157983543881e-07}\n",
      "Losses at iteration 107: {'ner': 1.9967468625225742}\n",
      "Losses at iteration 108: {'ner': 0.03327409187108807}\n",
      "Losses at iteration 109: {'ner': 0.0030281539282344862}\n",
      "Losses at iteration 110: {'ner': 0.0004471243265756487}\n",
      "Losses at iteration 111: {'ner': 7.212601973636832e-05}\n",
      "Losses at iteration 112: {'ner': 8.749638357961547e-06}\n",
      "Losses at iteration 113: {'ner': 0.0009516758637370681}\n",
      "Losses at iteration 114: {'ner': 4.308971797738075e-05}\n",
      "Losses at iteration 115: {'ner': 0.005126486441806077}\n",
      "Losses at iteration 116: {'ner': 0.0016268023538264181}\n",
      "Losses at iteration 117: {'ner': 0.05506942738122344}\n",
      "Losses at iteration 118: {'ner': 8.610277100330656e-06}\n",
      "Losses at iteration 119: {'ner': 5.165442502450185e-07}\n",
      "Losses at iteration 120: {'ner': 9.695990271014115e-05}\n",
      "Losses at iteration 121: {'ner': 2.6372600870713175e-05}\n",
      "Losses at iteration 122: {'ner': 5.962676971950593e-06}\n",
      "Losses at iteration 123: {'ner': 0.00010714553445003755}\n",
      "Losses at iteration 124: {'ner': 5.415092142193624e-10}\n",
      "Losses at iteration 125: {'ner': 0.0001605204071140701}\n",
      "Losses at iteration 126: {'ner': 2.955763026029226e-07}\n",
      "Losses at iteration 127: {'ner': 1.8058520189116317e-07}\n",
      "Losses at iteration 128: {'ner': 0.0021290899386661962}\n",
      "Losses at iteration 129: {'ner': 1.5022127782446888e-08}\n",
      "Losses at iteration 130: {'ner': 0.00027538781486174854}\n",
      "Losses at iteration 131: {'ner': 1.3729457061585413e-08}\n",
      "Losses at iteration 132: {'ner': 2.443167171762719e-07}\n",
      "Losses at iteration 133: {'ner': 2.2619411471954136e-05}\n",
      "Losses at iteration 134: {'ner': 3.0726581924385045e-08}\n",
      "Losses at iteration 135: {'ner': 2.524030315959595e-06}\n",
      "Losses at iteration 136: {'ner': 6.284484860065613e-05}\n",
      "Losses at iteration 137: {'ner': 0.0006337611946512826}\n",
      "Losses at iteration 138: {'ner': 0.3312746973043728}\n",
      "Losses at iteration 139: {'ner': 1.5791522298132958}\n",
      "Losses at iteration 140: {'ner': 5.387231094221414e-06}\n",
      "Losses at iteration 141: {'ner': 9.161237748484886e-06}\n",
      "Losses at iteration 142: {'ner': 0.0007062456316009719}\n",
      "Losses at iteration 143: {'ner': 0.0018581734749663437}\n",
      "Losses at iteration 144: {'ner': 1.9999934770956456}\n",
      "Losses at iteration 145: {'ner': 1.263028546996387e-08}\n",
      "Losses at iteration 146: {'ner': 3.8285358452678676e-06}\n",
      "Losses at iteration 147: {'ner': 2.420500751902158e-08}\n",
      "Losses at iteration 148: {'ner': 3.533395692569204e-08}\n",
      "Losses at iteration 149: {'ner': 0.4621877317614882}\n",
      "Losses at iteration 150: {'ner': 0.15511585537960454}\n",
      "Losses at iteration 151: {'ner': 9.192297318767554e-07}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses at iteration 152: {'ner': 1.5006812643122097e-06}\n",
      "Losses at iteration 153: {'ner': 0.36617435432415935}\n",
      "Losses at iteration 154: {'ner': 4.785157607960906e-10}\n",
      "Losses at iteration 155: {'ner': 2.0322615682867977}\n",
      "Losses at iteration 156: {'ner': 0.001551886632788568}\n",
      "Losses at iteration 157: {'ner': 0.012182387042223944}\n",
      "Losses at iteration 158: {'ner': 1.1616918644281524e-08}\n",
      "Losses at iteration 159: {'ner': 0.22900598879804238}\n",
      "Losses at iteration 160: {'ner': 9.498381067012785e-08}\n",
      "Losses at iteration 161: {'ner': 4.801329190902113e-09}\n",
      "Losses at iteration 162: {'ner': 0.00025933650643049914}\n",
      "Losses at iteration 163: {'ner': 2.630231781675002e-05}\n",
      "Losses at iteration 164: {'ner': 7.701292894871316e-05}\n",
      "Losses at iteration 165: {'ner': 7.70642589298445e-09}\n",
      "Losses at iteration 166: {'ner': 9.223506415247357e-07}\n",
      "Losses at iteration 167: {'ner': 0.001180002961483534}\n",
      "Losses at iteration 168: {'ner': 1.0185602398868738e-05}\n",
      "Losses at iteration 169: {'ner': 7.4086630298678595e-09}\n",
      "Losses at iteration 170: {'ner': 1.0019017107698989e-05}\n",
      "Losses at iteration 171: {'ner': 3.411818371404489e-08}\n",
      "Losses at iteration 172: {'ner': 1.830341121028215}\n",
      "Losses at iteration 173: {'ner': 5.919536899361234e-06}\n",
      "Losses at iteration 174: {'ner': 1.714151770535983}\n",
      "Losses at iteration 175: {'ner': 1.1738331256947429e-06}\n",
      "Losses at iteration 176: {'ner': 1.157237136051489}\n",
      "Losses at iteration 177: {'ner': 0.10278822591677392}\n",
      "Losses at iteration 178: {'ner': 4.42045789228087e-06}\n",
      "Losses at iteration 179: {'ner': 3.242968459395872e-06}\n",
      "Losses at iteration 180: {'ner': 0.02800043521868721}\n",
      "Losses at iteration 181: {'ner': 0.20182134426458806}\n",
      "Losses at iteration 182: {'ner': 1.3884784172090632}\n",
      "Losses at iteration 183: {'ner': 1.8758703782658627e-09}\n",
      "Losses at iteration 184: {'ner': 1.6554350296481708e-11}\n",
      "Losses at iteration 185: {'ner': 1.971730640444452e-06}\n",
      "Losses at iteration 186: {'ner': 1.9960251717896174}\n",
      "Losses at iteration 187: {'ner': 1.2912011025232826e-05}\n",
      "Losses at iteration 188: {'ner': 0.0011754159925290246}\n",
      "Losses at iteration 189: {'ner': 0.008036730780777499}\n",
      "Losses at iteration 190: {'ner': 3.9275808781991465e-06}\n",
      "Losses at iteration 191: {'ner': 1.957689249991267e-10}\n",
      "Losses at iteration 192: {'ner': 5.06222238078234e-10}\n",
      "Losses at iteration 193: {'ner': 0.0002638021419500205}\n",
      "Losses at iteration 194: {'ner': 0.03209198868914108}\n",
      "Losses at iteration 195: {'ner': 1.1899807723988491e-06}\n",
      "Losses at iteration 196: {'ner': 0.00011633974861110445}\n",
      "Losses at iteration 197: {'ner': 7.327266577708091e-05}\n",
      "Losses at iteration 198: {'ner': 7.18702219910377e-06}\n",
      "Losses at iteration 199: {'ner': 8.620242278988072e-08}\n",
      "Saved model to: ./ner_model\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.training import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "\n",
    "def train_ner(model_dir=\"./ner_model\", new_data=TRAIN_DATA, n_iter=10):\n",
    "    # Load the pre-trained \"en_core_web_sm\" model\n",
    "    if Path(model_dir).exists():\n",
    "        print(f\"Loading existing model from: {model_dir}\")\n",
    "        nlp = spacy.load(model_dir)  # Load the existing model\n",
    "    else:\n",
    "        print(\"Loading the pre-trained 'en_core_web_sm' model\")\n",
    "        nlp = spacy.load(\"en_core_web_sm\")  # Load the pre-trained model if none exists\n",
    "        if \"ner\" not in nlp.pipe_names:\n",
    "            ner = nlp.add_pipe(\"ner\", last=True)\n",
    "        else:\n",
    "            ner = nlp.get_pipe(\"ner\")\n",
    "        # Add new entity labels to the NER model\n",
    "        for _, annotations in new_data:\n",
    "            for ent in annotations.get(\"entities\"):\n",
    "                ner.add_label(ent[2])\n",
    "\n",
    "    # Continue training the model\n",
    "    if \"ner\" in nlp.pipe_names:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "    \n",
    "    # Disable other pipeline components during training to focus training on the NER component\n",
    "    with nlp.disable_pipes(*[pipe for pipe in nlp.pipe_names if pipe != \"ner\"]):\n",
    "        optimizer = nlp.resume_training()\n",
    "        for itn in range(n_iter):\n",
    "            losses = {}\n",
    "            batches = minibatch(new_data, size=compounding(4., 32., 1.001))\n",
    "            for batch in batches:\n",
    "                for text, annotations in batch:\n",
    "                    doc = nlp.make_doc(text)\n",
    "                    example = Example.from_dict(doc, annotations)\n",
    "                    nlp.update([example], drop=0.5, losses=losses, sgd=optimizer)\n",
    "            print(f\"Losses at iteration {itn}: {losses}\")\n",
    "\n",
    "    # Save the updated model\n",
    "    nlp.to_disk(model_dir)\n",
    "    print(f\"Saved model to: {model_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_ner()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18dfd53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \n",
      "like \n",
      "Fornite \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import json\n",
    "def predict(text):\n",
    "    nlp = spacy.load(\"./ner_model\")  # Ensure this path is correct\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        print(token.text, token.ent_type_)\n",
    "    for ent in doc.ents:\n",
    "        print(\"Entity:\", ent.text, ent.label_)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    predict_text = \"I like Fornite\"\n",
    "    predict_to_json(predict_text, \"prediction_output.json\")\n",
    "    predict(predict_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3f85fc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torch-2.2.1%2Bcu118-cp39-cp39-win_amd64.whl (2704.2 MB)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.17.1%2Bcu118-cp39-cp39-win_amd64.whl (4.9 MB)\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.2.1%2Bcu118-cp39-cp39-win_amd64.whl (4.0 MB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (2.7.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torchvision) (1.21.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "Successfully installed torch-2.2.1+cu118 torchaudio-2.2.1+cu118 torchvision-0.17.1+cu118\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55a4989a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f94af6e23164e1bb4cddd5c8b604b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3388255e0e74d8abbaa6846b019e409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b22f2ce2cb9748098f22ab20edfe292e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1377295edcb64773b95339a281d2d473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d436e4b6d68349e3815d25f214846796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The phrase most similar to 'move the ball' is: 'punt' with a similarity score of 0.4493\n",
      "Similarity to 'walk': 0.3580\n",
      "Similarity to 'pass': 0.3633\n",
      "Similarity to 'punt': 0.4493\n",
      "Similarity to 'run': 0.3016\n",
      "Similarity to 'kick': 0.4475\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "# Simulated dataset: Normally, you'd load a dataset from a file\n",
    "data = [\n",
    "    {\"sentence1\": \"move the ball\", \"sentence2\": \"pass the ball\", \"similarity\": 0.9},\n",
    "    {\"sentence1\": \"move the ball\", \"sentence2\": \"walk\", \"similarity\": 0.1},\n",
    "    {\"sentence1\": \"move the ball\", \"sentence2\": \"punt the ball\", \"similarity\": 0.8},\n",
    "    {\"sentence1\": \"move the ball\", \"sentence2\": \"run\", \"similarity\": 0.2},\n",
    "]\n",
    "\n",
    "# Convert the simulated dataset to a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert the DataFrame to a list of InputExample objects\n",
    "examples = [InputExample(texts=[row['sentence1'], row['sentence2']], label=row['similarity']) for index, row in df.iterrows()]\n",
    "\n",
    "# Initialize DataLoader\n",
    "train_dataloader = DataLoader(examples, shuffle=True, batch_size=2)\n",
    "\n",
    "# Initialize the model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Define the training method using cosine similarity loss\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "\n",
    "# Assuming a very small dataset, let's skip validation for this example.\n",
    "# In a real scenario, you should split your data and use a validation set.\n",
    "\n",
    "# Fine-tune the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          epochs=4,  # Adjust epochs based on your dataset size and complexity\n",
    "          warmup_steps=100,\n",
    "          output_path=\"./fine-tuned-model\"  # Change this to where you want to save your model\n",
    "         )\n",
    "\n",
    "# Load the fine-tuned model (optional if you continue using the same model object)\n",
    "model = SentenceTransformer(\"./fine-tuned-model\")\n",
    "\n",
    "# Now you can use the model as before to generate embeddings and calculate similarities\n",
    "# The target phrase and phrases to compare\n",
    "target_phrase = \"move the ball\"\n",
    "phrases = [\"walk\", \"pass\", \"punt\", \"run\",\"kick\"]\n",
    "\n",
    "# Generate embeddings for each phrase\n",
    "target_embedding = model.encode(target_phrase)\n",
    "phrase_embeddings = model.encode(phrases)\n",
    "\n",
    "# Calculate and print the cosine similarity between the target and each phrase\n",
    "similarities = {}\n",
    "for phrase, embedding in zip(phrases, phrase_embeddings):\n",
    "    # Compute cosine similarity (note: 1 - cosine distance to get similarity)\n",
    "    similarity = 1 - cosine(target_embedding, embedding)\n",
    "    similarities[phrase] = similarity\n",
    "\n",
    "# Find the most similar phrase\n",
    "most_similar_phrase = max(similarities, key=similarities.get)\n",
    "print(f\"The phrase most similar to '{target_phrase}' is: '{most_similar_phrase}' with a similarity score of {similarities[most_similar_phrase]:.4f}\")\n",
    "\n",
    "# Optional: print all similarities for comparison\n",
    "for phrase, similarity in similarities.items():\n",
    "    print(f\"Similarity to '{phrase}': {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7c7953",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

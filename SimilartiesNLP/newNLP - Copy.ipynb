{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c1e686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.training import Example\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "from spacy.util import filter_spans\n",
    "import random\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "from train_data import TRAIN_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25c1a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing model from: ./ner_model\n"
     ]
    }
   ],
   "source": [
    "def train_ner(model_dir=\"./ner_model\", new_data=TRAIN_DATA, n_iter=10):\n",
    "    # Load the pre-trained \"en_core_web_sm\" model\n",
    "    if Path(model_dir).exists():\n",
    "        print(f\"Loading existing model from: {model_dir}\")\n",
    "        nlp = spacy.load(model_dir)  # Load the existing model\n",
    "    else:\n",
    "        print(\"Loading the pre-trained 'en_core_web_sm' model\")\n",
    "        nlp = spacy.load(\"en_core_web_sm\")  # Load the pre-trained model if none exists\n",
    "        if \"ner\" not in nlp.pipe_names:\n",
    "            ner = nlp.add_pipe(\"ner\", last=True)\n",
    "        else:\n",
    "            ner = nlp.get_pipe(\"ner\")\n",
    "        # Add new entity labels to the NER model\n",
    "        for _, annotations in new_data:\n",
    "            for ent in annotations.get(\"entities\"):\n",
    "                ner.add_label(ent[2])\n",
    "\n",
    "    # Continue training the model\n",
    "    if \"ner\" in nlp.pipe_names:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "    \n",
    "    # Disable other pipeline components during training to focus training on the NER component\n",
    "    with nlp.disable_pipes(*[pipe for pipe in nlp.pipe_names if pipe != \"ner\"]):\n",
    "        optimizer = nlp.resume_training()\n",
    "        for itn in range(n_iter):\n",
    "            losses = {}\n",
    "            batches = minibatch(new_data, size=compounding(4., 32., 1.001))\n",
    "            for batch in batches:\n",
    "                for text, annotations in batch:\n",
    "                    doc = nlp.make_doc(text)\n",
    "                    example = Example.from_dict(doc, annotations)\n",
    "                    nlp.update([example], drop=0.5, losses=losses, sgd=optimizer)\n",
    "            print(f\"Losses at iteration {itn}: {losses}\")\n",
    "\n",
    "    # Save the updated model\n",
    "    nlp.to_disk(model_dir)\n",
    "    print(f\"Saved model to: {model_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_ner()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18dfd53e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_to_json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     12\u001b[0m     predict_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI like Fornite\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 13\u001b[0m     \u001b[43mpredict_to_json\u001b[49m(predict_text, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction_output.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m     predict(predict_text)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predict_to_json' is not defined"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import json\n",
    "def predict(text):\n",
    "    nlp = spacy.load(\"./ner_model\")  # Ensure this path is correct\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        print(token.text, token.ent_type_)\n",
    "    for ent in doc.ents:\n",
    "        print(\"Entity:\", ent.text, ent.label_)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    predict_text = \"I like Fornite\"\n",
    "    predict_to_json(predict_text, \"prediction_output.json\")\n",
    "    predict(predict_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3f85fc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torch-2.2.1%2Bcu118-cp39-cp39-win_amd64.whl (2704.2 MB)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.17.1%2Bcu118-cp39-cp39-win_amd64.whl (4.9 MB)\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.2.1%2Bcu118-cp39-cp39-win_amd64.whl (4.0 MB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (2.7.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torchvision) (1.21.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "Successfully installed torch-2.2.1+cu118 torchaudio-2.2.1+cu118 torchvision-0.17.1+cu118\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55a4989a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c28e73b9af7f481380cbbf80a9cd27bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "126e751ffb3146b2a8d2bc3fae08493f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f46710859f545a2a46af2f2117be8f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "586eb0cb77cd453ebe3caaeabdc83c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c29d7aadc64b1b9166409058443e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The phrase most similar to 'move the ball' is: 'punt' with a similarity score of 0.4584\n",
      "Similarity to 'walk': 0.3673\n",
      "Similarity to 'pass': 0.3767\n",
      "Similarity to 'punt': 0.4584\n",
      "Similarity to 'run': 0.3102\n",
      "Similarity to 'kick': 0.4556\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from sentence_transformers import evaluation\n",
    "\n",
    "# Simulated dataset: Normally, you'd load a dataset from a file\n",
    "data = [\n",
    "    {\"sentence1\": \"move the ball\", \"sentence2\": \"pass\", \"similarity\": 0.9},\n",
    "    {\"sentence1\": \"move the ball\", \"sentence2\": \"walk\", \"similarity\": 0.1},\n",
    "    {\"sentence1\": \"move the ball\", \"sentence2\": \"punt\", \"similarity\": 0.8},\n",
    "    {\"sentence1\": \"move the ball\", \"sentence2\": \"run\", \"similarity\": 0.2},\n",
    "]\n",
    "\n",
    "# Convert the simulated dataset to a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert the DataFrame to a list of InputExample objects\n",
    "examples = [InputExample(texts=[row['sentence1'], row['sentence2']], label=row['similarity']) for index, row in df.iterrows()]\n",
    "\n",
    "# Initialize DataLoader\n",
    "train_dataloader = DataLoader(examples, shuffle=True, batch_size=2)\n",
    "\n",
    "# Initialize the model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Define the training method using cosine similarity loss\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "\n",
    "# Assuming a very small dataset, let's skip validation for this example.\n",
    "# In a real scenario, you should split your data and use a validation set.\n",
    "\n",
    "# Fine-tune the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          epochs=4,  # Adjust epochs based on your dataset size and complexity\n",
    "          warmup_steps=100,\n",
    "          output_path=\"./fine-tuned-model\"  # Change this to where you want to save your model\n",
    "         )\n",
    "\n",
    "# Load the fine-tuned model (optional if you continue using the same model object)\n",
    "model = SentenceTransformer(\"./fine-tuned-model\")\n",
    "\n",
    "# Now you can use the model as before to generate embeddings and calculate similarities\n",
    "# The target phrase and phrases to compare\n",
    "target_phrase = \"kick the ball\"\n",
    "phrases = [\"walk\", \"pass\", \"punt\", \"run\"]\n",
    "phrases_button = [\"A\",\"X\",\"Y\",\"B\"]\n",
    "\n",
    "# Generate embeddings for each phrase\n",
    "target_embedding = model.encode(target_phrase)\n",
    "phrase_embeddings = model.encode(phrases)\n",
    "\n",
    "# Calculate and print the cosine similarity between the target and each phrase\n",
    "similarities = {}\n",
    "for phrase, embedding in zip(phrases, phrase_embeddings):\n",
    "    # Compute cosine similarity (note: 1 - cosine distance to get similarity)\n",
    "    similarity = 1 - cosine(target_embedding, embedding)\n",
    "    similarities[phrase] = similarity\n",
    "\n",
    "# Find the most similar phrase\n",
    "most_similar_phrase = max(similarities, key=similarities.get)\n",
    "print(f\"The phrase most similar to '{target_phrase}' is: '{most_similar_phrase}' with a similarity score of {similarities[most_similar_phrase]:.4f}\")\n",
    "\n",
    "# Optional: print all similarities for comparison\n",
    "for phrase, similarity in similarities.items():\n",
    "    print(f\"Similarity to '{phrase}': {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea7c7953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # Loading the model\n",
    "\n",
    "def get_similar_words(word, topn=5):\n",
    "    # Process the word to get its vector\n",
    "    queried_token = nlp(word)[0]\n",
    "    \n",
    "    if queried_token.has_vector:\n",
    "        similarities = []\n",
    "        for token in nlp.vocab:\n",
    "            # Check if the token has a vector and matches the casing\n",
    "            if token.has_vector and token.is_lower == queried_token.is_lower:\n",
    "                similarity = queried_token.similarity(token)\n",
    "                if similarity > 0.2:  # Threshold for similarity\n",
    "                    similarities.append((token.text, similarity))\n",
    "        similarities = sorted(similarities, key=lambda item: item[1], reverse=True)\n",
    "        return [word for word, similarity in similarities[:topn]]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Example usage\n",
    "similar_words = get_similar_words(\"hello\")\n",
    "print(similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9700af6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to play FIFA with my body. I want my right hand to control movement of the player. I want to put my fist up to pass the ball. I want to sprint when I show three finger. I also want to kick the ball when I Kick In Real Life.\n",
      "7.02576208114624\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "from deepmultilingualpunctuation import PunctuationModel\n",
    "model = PunctuationModel()\n",
    "text = \"I want to play FIFA with my body I want my right hand to control movement of the player I want to put my fist up to pass the ball I want to sprint when I show three finger I also want to kick the ball when I Kick In Real Life\"\n",
    "result = model.restore_punctuation(text)\n",
    "print(result)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce672fcb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'python' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpython\u001b[49m \u001b[38;5;241m-\u001b[39mV\n",
      "\u001b[1;31mNameError\u001b[0m: name 'python' is not defined"
     ]
    }
   ],
   "source": [
    "python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe75f4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: conda-script.py [-h] [-V] command ...\n",
      "\n",
      "conda is a tool for managing and deploying applications, environments and packages.\n",
      "\n",
      "Options:\n",
      "\n",
      "positional arguments:\n",
      "  command\n",
      "    clean        Remove unused packages and caches.\n",
      "    compare      Compare packages between conda environments.\n",
      "    config       Modify configuration values in .condarc. This is modeled\n",
      "                 after the git config command. Writes to the user .condarc\n",
      "                 file (C:\\Users\\ASUS\\.condarc) by default.\n",
      "    create       Create a new conda environment from a list of specified\n",
      "                 packages.\n",
      "    help         Displays a list of available conda commands and their help\n",
      "                 strings.\n",
      "    info         Display information about current conda install.\n",
      "    init         Initialize conda for shell interaction. [Experimental]\n",
      "    install      Installs a list of packages into a specified conda\n",
      "                 environment.\n",
      "    list         List linked packages in a conda environment.\n",
      "    package      Low-level conda package utility. (EXPERIMENTAL)\n",
      "    remove       Remove a list of packages from a specified conda environment.\n",
      "    uninstall    Alias for conda remove.\n",
      "    run          Run an executable in a conda environment.\n",
      "    search       Search for packages and display associated information. The\n",
      "                 input is a MatchSpec, a query language for conda packages.\n",
      "                 See examples below.\n",
      "    update       Updates conda packages to the latest compatible version.\n",
      "    upgrade      Alias for conda update.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help     Show this help message and exit.\n",
      "  -V, --version  Show the conda version number and exit.\n",
      "\n",
      "conda commands available from other packages:\n",
      "  build\n",
      "  content-trust\n",
      "  convert\n",
      "  debug\n",
      "  develop\n",
      "  env\n",
      "  index\n",
      "  inspect\n",
      "  metapackage\n",
      "  pack\n",
      "  render\n",
      "  repo\n",
      "  server\n",
      "  skeleton\n",
      "  token\n",
      "  verify\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906bab62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
